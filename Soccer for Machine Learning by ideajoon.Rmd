---
title: "Assignment3 includes Assignment1,2"
author: "KIM MIN JOON 20196004"
date: "2019³â 6¿ù 2ÀÏ"
output: html_document
---



# Statistical Learning for Soccer.

## Assignment 1 : Naive bayes classifier
Developing a system that can predict the outcome of a soccer match.

## Assignment 2 : Feature Selection Regularizer
Using a regression model, we create a service that recommends soccer positions to clients.

## Assignment 3 : Time Series Analytics
The objective of this project is to forecast the number of natinoal soccer match in next three years. 





# =======================================
date: '2019-04-07'
# Assingment #1 (Naive bayes classifier)

```{r include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
Sys.setlocale('LC_ALL','C')
```

# 1. Objectives
Developing a system that can predict the outcome of a soccer match.

# 2. Hypothesis
The outcome of a match can be predicted by the statistical data of first half-time.
(= This data will affects the result of a soccer match)  


# 3. Methods 
Use the R programming to learn train-data and predict the outcome of test-data 
with Naive Bayes Classifier


# 4. Data preprocessing
## 4-1. LOAD A DATASET.
This dataset includes 3,719 instances and 1 class, 14 features.
This dataset contains data for last 10 seasons of English Premier League including current season. 
The data is sourced from http://www.football-data.co.uk/ website and contains various statistical data
such as final and half time result, corners, yellow and red cards etc.

```{r}
# load the dataset
match.data <- read.csv(file="C://Users//joon//Documents//10years season-0919-v3.csv", header = TRUE)
head(match.data)
```


## 4-2. Prediction with Naive Bayes Classification.
The e1071 package contains a function named naiveBayes() which is performing Bayes classification.
The model is trained on training dataset[data_train] to make predictions by predict() function.

```{r}
# load the library
library(e1071)
library(caret)

# define an 80%/20% train/test split of the datase
set.seed(3333)
split=0.70
trainIndex <- createDataPartition(match.data$FTR, p=split, list=FALSE)
data_train <- match.data[trainIndex, ]
data_test <- match.data[-trainIndex, ]

# train a naive bayes model
model <- naiveBayes(FTR ~ ., data = data_train)

# make predictions
x_test <- data_test[,-1]
y_test <- data_test[,1]
pred <- predict(model, x_test)

# summarize results, include table(pred, y_test)
confusionMatrix(pred, y_test)
table(pred, y_test)
```


## 4-3. Check Missing Data
```{r}
# count of feature's null
sum(is.na(match.data[,1:15]))
```


# 5. Exploratory Data Analysis
## 5-1. correlation matrix plot
A default correlation matrix plot (called a Correlogram) is generated. 
Positive correlations are displayed in a blue scale while negative correlations are displayed in a red scale.
```{r}
# correlation matrix plot
library(corrplot)
match.data_cor <- cor(match.data[,-1])
corrplot(match.data_cor, 
         method="shade", 
         addshade="all",
         tl.col="red", 
         tl.srt=30, 
         diag=FALSE, 
         addCoef.col="black", 
         order="FPC"
        )

# scatter-plot matrix, correlation, histogram
library(psych)
pairs.panels(match.data[,-1])
```


## 5-2. Recheck correalation after removing 'HS, AS' columns with high correlation
```{r}
# Remove columns with high correlation
match2.data <- match.data[,-4:-5]

# Correlation matrix plot
match2.data_cor <- cor(match2.data[,-1])
corrplot(match2.data_cor, method="shade", addshade="all", tl.col="red", tl.srt=30, 
         diag=FALSE, addCoef.col="black", order="FPC" 
        )

# scatter-plot matrix, correlation, histogram
library(psych)
pairs.panels(match2.data[,-1])
```

# 6. Select of Optimal Models and Visualization.
## 6-1. Reprediction with match2.data (Naive Bayes Classification)
```{r}

# define an 70%/30% train/test split of the datase
set.seed(3333)
split=0.70
trainIndex2 <- createDataPartition(match2.data$FTR, p=split, list=FALSE)
data_train2 <- match2.data[trainIndex2, ]
data_test2 <- match2.data[-trainIndex2, ]

# train a naive bayes model
model2 <- naiveBayes(FTR ~ ., data = data_train2)

# make predictions
x_test2 <- data_test2[,-1]
y_test2 <- data_test2[,1]
pred2 <- predict(model2, x_test2)

# summarize results, include table(pred2, y_test2)
confusionMatrix(pred2, y_test2)
table(pred2, y_test2)
```



## 6-2. 'match2.data' data Visualization
```{r}
library(dplyr) # %>% 'pipeline' package
library(tidyr) # gather package

# check variable's distribution with histogram
match2.data %>%
  gather(name, value, 2:13, -FTR) %>% 
  ggplot(aes(value, fill = name)) + 
  geom_histogram(show.legend = FALSE) + 
  facet_wrap(~ name, scales = "free")

# check variable's distribution with boxplot
match2.data %>%
  gather(name, value, 2:13, -FTR) %>% 
  ggplot(aes(y=value, x=FTR, fill = FTR)) + 
  geom_boxplot(show.legend = TRUE) + 
  facet_wrap(~ name, scales = "free")

```


## 6-3. After convert integer columns to factor columns
```{r}
# change to Categorical variable
str(match2.data)
match3.data <- match2.data
match3.data$AR <- as.factor(match3.data$AR)
match3.data$HR <- as.factor(match3.data$HR)
str(match3.data)
```


## 6-4. Reprediction for match3.data with Naive Bayes Classification
```{r}

# define an 70%/30% train/test split of the datase
set.seed(3333)
split=0.70
trainIndex3 <- createDataPartition(match3.data$FTR, p=split, list=FALSE)
data_train3 <- match3.data[trainIndex3, ]
data_test3 <- match3.data[-trainIndex3, ]

# train a naive bayes model
model3 <- naiveBayes(FTR ~ ., data = data_train3)

# make predictions
x_test3 <- data_test3[,-1]
y_test3 <- data_test3[,1]
pred3 <- predict(model3, x_test3)

# summarize results, include table(pred3, y_test3)
confusionMatrix(pred3, y_test3)
table(pred3, y_test3)
```


## 6-5. Change to binomial Classification
```{r}
# multi Classvalue -> binary Classvalue
a <- gsub("H","win",match2.data$FTR)
b <- gsub("D","nowin",a)
newclass <- gsub("A","nowin",b)
match4.data <- match2.data
match4.data$FTR <- as.factor(newclass)
str(match2.data)
str(match4.data)
head(match4.data)
tail(match4.data)
```


## 6-6. Reprediction for match4.data with Naive Bayes Classification
```{r}
# define an 70%/30% train/test split of the datase
set.seed(3333)
split=0.70
trainIndex4 <- createDataPartition(match4.data$FTR, p=split, list=FALSE)
data_train4 <- match4.data[trainIndex4, ]
data_test4 <- match4.data[-trainIndex4, ]

# train a naive bayes model
model4 <- naiveBayes(FTR ~ ., data = data_train4)

# make predictions
x_test4 <- data_test4[,-1]
y_test4 <- data_test4[,1]
pred4 <- predict(model4, x_test4)

# summarize results, include table(pred4, y_test4)
confusionMatrix(pred4, y_test4)
table(pred4, y_test4)


```



## 6-7. Calculation AUC (Area Under Curve) and Plot ROC Curve (Receiver Operating Characteristic Curve)
```{r}
#Calculate AUC
library(pROC)
library(MASS)

Diag_DF <- data.frame(Attribute=c(colnames(match4.data)[2:13]), AUC=NA)   # create dataframe for AUC 
for(i in 1:nrow(Diag_DF)){
  roc_result <- roc(match4.data$FTR, match4.data[,as.character(Diag_DF$Attribute[i])])   
  Diag_DF[i,'AUC'] <- roc_result$auc  
}
Diag_DF <- Diag_DF[order(-Diag_DF$AUC),]  
Diag_DF 

#ROC curve plot
HTHG_roc <- roc(match4.data$FTR, match4.data$HTHG)  
plot.roc(HTHG_roc,  
         col="red",  
         print.auc=TRUE, print.auc.adj=c(1.6,-8), 
         max.auc.polygon=TRUE, print.thres.adj=c(0.3,-1.2),
         print.thres=TRUE, print.thres.pch=19, print.thres.col = "red",  
         auc.polygon=TRUE, auc.polygon.col="#D1F2EB")

AST_roc <- roc(match4.data$FTR, match4.data$AST) 
plot.roc(AST_roc,  
         add=TRUE,  
         col="blue",  
         print.auc=TRUE, print.auc.adj=c(0.3, 0.2), 
         print.thres=TRUE, print.thres.pch=19, 
         print.thres.col = "blue", print.thres.adj=c(-0.085,1.1)) 

legend("bottomright",  
       legend=c("HTHG", "AST"),  
       col=c("red", "blue"), lwd=2) 

```



## 6-8. 'data_train4' data Visualization
```{r}
library(dplyr) # %>% 'pipeline' package
library(tidyr) # gather package

# check variable's distribution with density
data_train4 %>%
  gather(name, value, 2:13, -FTR) %>% 
  ggplot(aes(value, fill = FTR)) + 
  geom_density(alpha = 0.5, show.legend = TRUE) + 
  facet_wrap(~ name, scales = "free")

# check variable's distribution with boxplot
data_train4 %>%
  gather(name, value, 2:13, -FTR) %>% 
  ggplot(aes(y=value, x=FTR, fill = FTR)) + 
  geom_boxplot(show.legend = TRUE) + 
  facet_wrap(~ name, scales = "free")

```



## 6-9. Model K-Fold Cross-Validation Ver1
The caret package contains train() function which is performing navie bayes classification and K-fold cross-validation.
The trainControl argument tells the trainer to use cross-validation ('cv') with 10 folds.
```{r}
# load the library
library(caret)

# K-fold Cross Validation
x <- match4.data[,-1]
y <- match4.data$FTR
model5 <- train(x,y,method='nb',trControl = trainControl(method='cv', number = 10, savePredictions = TRUE))

confusionMatrix(model5)
cat("Accuracy(k=10) = ", model5$resample$Accuracy)
```


## 6-10. Model K-Fold Cross-Validation Ver2
Without using 'caret' package's train() function, apply K-fold cv with cording.
```{r}

# Create k equally size folds
set.seed(1122)
match5.data <- match4.data[sample(nrow(match4.data)),]
k <- 10
folds <- cut(seq(1,nrow(match5.data)),breaks=k,labels=FALSE)

# Perform k fold cross validation
m_result <- data.frame(matrix(nrow=10, ncol=6))
for(i in 1:k){
  #Segement your data by fold using the which() function 
  trainIndex5 <- which(folds==i,arr.ind=TRUE)
  data_test5 <- match5.data[trainIndex5, ]
  data_train5 <- match5.data[-trainIndex5, ]
 
  # train a naive bayes model
  model5 <- naiveBayes(FTR ~ ., data = data_train5)

  # make predictions
  x_test5 <- data_test5[,-1]
  y_test5 <- data_test5[,1]
  pred5 <- predict(model5, x_test5)


  # create the confusion matrix
  cm <- as.matrix(table(Predicted=pred5, Actual=y_test5))

  # Performance Measure of ML
  TN <- cm[1,1] # True Negative
  FN <- cm[1,2] # False Negative
  FP <- cm[2,1] # False Positive
  TP <- cm[2,2] # True Positive
  TPR <- TP/(TP+FN) # True Positive Rate
  FPR <- FP/(FP+TN) # False Psitive Rate

  # Accuracy
  accuracy <- (TP+TN)/(TP+FP+FN+TN)

  # Sensitivity, Specificity
  sensitivity <- TPR
  specificity <- 1-FPR

  # Per-class Precision, Recall, and F-1
  precision <- TP/(TP+FP)
  recall <- TP/(TP+FN) 
  f1 <- 2 * precision * recall / (precision + recall)

  
  m_result[i,] <- data.frame(accuracy, sensitivity, specificity, precision, recall, f1)
  colnames(m_result) <- c("accuracy", "sensitivity", "specificity", "precision", "recall", "f1")
  rownames(m_result) <- c(1:k)
}

# pirint result
print(m_result)
print(apply(m_result, 2, mean))
 
```


```{r}
library(caret)

# K-fold Cross Validation
k <- c(3, 5, 10, 30, 60, 120)
Accuracydata <- data.frame(nrow=6, ncol=2)
for(i in k) {
  x2 <- match4.data[,-1]
  y2 <- match4.data$FTR
  model6 <- train(x2,y2,method='nb',trControl = trainControl(method='cv', number = i, savePredictions = TRUE))
  
  if (i==3) {
            k3 <- model6$resample$Accuracy 
            } else if (i==5) {
            k5 <- model6$resample$Accuracy
            } else if (i==10) {
            k10 <- model6$resample$Accuracy
            } else if (i==30) {
            k30 <- model6$resample$Accuracy
            } else if (i==60) {
            k60 <- model6$resample$Accuracy
            } else if (i==120) {
            k120 <- model6$resample$Accuracy
            }
}

# check Accuracy distribution by the number of k
boxplot(k3, k5, k10, k30, k60, k120, col=c("orange", "yellow", "green", "pink", "gray", "yellowgreen"),ylab="Accuracy", xlab="the number of k", names = c("k3", "k5", "k10", "k30", "k60", "k120"), main = "Accuracy distribution by the number of k")

```


# 7. Summary
## 7-1. Better Naive Bayes¡¯s performance
To increase performance, we need to check missing data, 
remove redundant features, change data type and level of class variable.
For instance, if the data contains highly correlated features,
the performance of Naive Bayes can degrade.
So we removed those features that are the most highly correlated.
In the end, we got an accuracy of 0.7548 (the initial accuracy of 0.6275)

## 7-2. Relationship between K and Bias and Variance
Remember, if K is small, the evaluation of the model will not be accurate.
But if K is higher, we will get lower bias, higher variance for the evaluation.
So we should focus on achieving a balance between bias and variance.


# 8. Conclusion
We recommend the k-fold cross-validation to estimate the prediction error rate.
When choosing K number, consider reducing the variance and controlling bias. 
Binary classification is better than multi classification for Naive bayes classifier.
And highly correlated features should be removed for good performance . 











# =======================================
date: "2019-5-12"
# Assingment #2 (Feature Selection Regularizer)


# 1. Objectives(Business problem)
Using a regression model, we create a service that recommends soccer positions to clients.

# 2. Hypothesis
The soccer position can be predicted With position dataset of European soccer players in the last ten years.

# 3. Select Optimal Dataset
## 3-1. Dataset Description
The dataset includes 4184 instances and 1 response variable and 40 independent variables.
This dataset contains data for soccer position information of European 4,184 soccer players in the last 10 years.
The data is sourced from https://www.kaggle.com/hugomathien/soccer website and 
contains various data such as below

1)position_numeric Æ÷Áö¼ÇÀ» ¼öÄ¡·Î Ç¥Çö
2)position_description Æ÷Áö¼Ç »ó¼¼ ¼³¸í
3)position_name Æ÷Áö¼Ç ¾à¾î ¸íÄª
4)height Å°
5)weight ¸ö¹«±âlb
6)overall_rating ÀüÃ¼
7)potential ÀáÀç´É·Â
8)preferred_foot ¿Þ¹ßÀâÀÌ? ¿À¸¥¹ßÀâÀÌ?
9)crossing º¼ Å©·Î½º ´É·Â
10)finishing ¸¶¹«¸® ´É·Â
11)heading_accuracy
12)short_passing
13)volleys ¹ß¸®Å± ´É·Â
14)dribbling
15)curve È¸Àü Å± ´É·Â
16)free_kick_accuracy ÇÁ¸®Å± Á¤È®µµ
17)long_passing
18)ball_control
19)acceleration ÁúÁÖ °¡¼Óµµ
20)sprint_speed ÁúÁÖ ¼Óµµ
21)agility ¹ÎÃ¸¼º
22)reactions ¹ÝÀÀ ´É·Â
23)balance ³Ñ¾îÁöÁö ¾Ê´Â
24)shot_power
25)jumping Á¡ÇÁ´É·Â
26)stamina Ã¼·Â
27)strength Èû
28)long_shots ·Õ½¸ ´É·Â
29)aggression °ø°Ý¼º
30)interceptions º¼ °¡·ÎÃ¤±â ´É·Â
31)positioning À§Ä¡ ¼±Á¤ ´É·Â
32)vision ½Ã¾ß
33)penalties Æä³ÎÆ®Å± ¼º°ø·ü
34)marking ¸¶Å· ´É·Â
35)standing_tackle
36)sliding_tackle
37)gk_diving Å°ÆÛ ´ÙÀÌºù ´É·Â
38)gk_handling Å°ÆÛ °øÅÍÄ¡°¨
39)gk_kicking Å°ÆÛ Å±·Â
40)gk_positioning Å°ÆÛ À§Ä¡¼±Á¤·Â
41)gk_reflexes Å°ÆÛ ¹ÝÀÀ·Â


## 3-2. Numeric Position's graph
```{r relu, echo=FALSE, fig.cap="A caption", out.width = '100%'}
knitr::include_graphics("C://Users//joon//Documents//numeric_soccer_position.png")
```


# 4. Data preprocessing

## 4-1 LOAD A DATASET.
```{r}
# load the dataset
getwd()
position.data <- read.csv(file="C://Users//joon//Documents//soccer_player_positions - v6.csv", header = TRUE, fileEncoding = "euc-kr")
# head(position.data)
str(position.data)
```


## 4-2 Remove Missing Value
```{r}
table(is.na.data.frame(position.data))

library(dplyr) # for removing missing value
position1.data <- na.omit(position.data) #Extracts missing data from all variables

table(is.na(position1.data))
```


## 4-3 Remove unnecessary categorical variables. 
```{r}
# relationship between position_name and numeric
rel_position <- position1.data[,c(1:3)] 

# Remove categorical variables and meaningless variables.
position1.data <- position1.data[,-c(2,3,6,7,33)]

# Removed categorical variables
# 2)position_description Æ÷Áö¼Ç »ó¼¼ ¼³¸í
# 3)position_name Æ÷Áö¼Ç ¾à¾î ¸íÄª

# Removed meaningless variables
# 6)overall_rating ÀüÃ¼
# 7)potential ÀáÀç´É·Â
# 33)penalties Æä³ÎÆ®Å± ¼º°ø·ü

```


## 4-4 Convert factor to numeric variable
```{r}
# convert factor(preferred_foot) to numeric variable 
# left_foot = 1, right_foot = 2
position1.data[,4] <- as.numeric(position1.data[,4])
```


# 5. Exploratory Data Analysis

## 5-1 linear regression model
```{r}
require(leaps)
# linear regression model
fit.lm <- lm(position_numeric~.,data=position1.data)
summary(fit.lm)
```
The null hypothesis: The regression model is not valid.
The alternative hypothesis: The regression model is valid.
Result : Since the p value is 0.0000, a rare model is reasonable at the significance level of 0.05.


## 5-2 Normality check
- null hypothesis: Follow the normal distribution.
- alternative hypothesis: It does not follow the normal distribution.
At a significant level of 0.05 p, the regression model is not normally distributed.
```{r}
library(car)
shapiro.test(fit.lm$residuals)
```

## 5-3 Independence check
- null hypothesis : Each error is independent.
- alternative hypothesis : Each error is not independent.
If the D-W statistic is close to 2, it is not self-correlated (each independent), or if it is further away from 2, 
it is self-correlated (each is not independent).
p-values do not meet the independence assumption of the regression model at a significant level of 0.05.
```{r}
car::durbinWatsonTest(fit.lm)
```

## 5-4 Heteroscedasticity check
- null hypothesis : Each error is Homogeneity of variance.
- alternative hypothesis : Each error is not Homogeneity of variance.(it is Heteroscedasticity of variance)
p°ªÀÌ À¯ÀÇ¼öÁØ 0.05¿¡¼­ Èñ±Í¸ðÇüÀº ¿¡·¯µéÀº µîºÐ»ê¼ºÀ» ¸¸Á·ÇÏÁö ¾Ê´Â´Ù.
```{r}
car::ncvTest(fit.lm)
```

## 5-5 Overall verification of linear model assumptions
- Global Stat : Overall satisfaction with errors, assumtions acceptable : generally satisfied
- Skewness(ºñ´ëÄªµµ), Kurtosis(Ã·µµ) : Normality test
- Link Funtion : Linearity test
- Heteroscedasticity : Heteroscedasticity test
At a significant level of 0.05 p of Global Stat, the regression model does not meet the assumption of a linear model.
```{r}
library(gvlma)
gvmodel<-gvlma(fit.lm)
summary(gvmodel)
fit.lm_glovalstat_pvalue <- gvmodel$GlobalTest$GlobalStat4$pvalue
```

## 5-6 Multicolinearity check
```{r}
sqrt(vif(fit.lm)) > 2
```


## 5-7 correlation matrix plot
A default correlation matrix plot (called a Correlogram) is generated. 
Positive correlations are displayed in a blue scale while negative correlations are displayed in a red scale.
```{r}
# scatter-plot matrix, correlation, histogram
library(psych)
pairs.panels(position1.data[,c(2:8)])
pairs.panels(position1.data[,c(9:15)])
pairs.panels(position1.data[,c(16:22)])
pairs.panels(position1.data[,c(23:29)])
pairs.panels(position1.data[,c(30:36)])
```


# 6. Select of Optimal Models and Visualization.
Apply dataset to some regression models and select of Optimal Models and Visualization of Model.

## 6-1 Create Training and Test Samples
```{r}
library(caret)

set.seed(1222)
split=0.70
trainIndex <- createDataPartition(position1.data$position_numeric, p=split, list=FALSE)
data_train <- position1.data[trainIndex, ]
data_test <- position1.data[-trainIndex, ]

x_train <- data_train[,-1]
y_train <- data_train[,1]

x_test <- data_test[,-1]
y_test <- data_test[,1]

x1 <- as.matrix(x_train)
y1 <- y_train

x2 <- as.matrix(x_test)
y2 <- y_test
```

## 6-2 Linear Regression
```{r}
# Linear Regresstion for train dataset
fit.lm <- lm(position_numeric~.,data=data_train)

lm.prediction <- predict(fit.lm, newx = x_test)

summary(fit.lm)
```


## 6-3 Backward Regression
The number of independent variable : 35 -> 26
```{r}
# starting from full model
full.model <- lm(position_numeric~.,data=data_train)
fit.bwd <- step(full.model,direction="backward", trace=0)

bwd.prediction <- predict(fit.bwd, newx = x_test)

summary(fit.bwd)
```

## 6-4 Forward Regression
The number of independent variable : 35 -> 26
```{r}
# starting from null model
min.model <- lm(position_numeric ~ 1, data=data_train)
biggest <- formula(lm(position_numeric~., position1.data))
fit.fwd <- step(min.model, direction="forward", scope=biggest, trace=0)

fwd.prediction <- predict(fit.fwd, newx = x_test)

summary(fit.fwd)
```

## 6-5 Stepwise Regression
The number of independent variable : 35 -> 26
```{r}
min.model <- lm(position_numeric ~ 1,data=data_train)
full.model <- lm(position_numeric~.,data=data_train)

fit.stepwise <- step(min.model, scope = list(upper=full.model), direction = "both", data=data_train, trace=0)

stepwise.prediction <- predict(fit.stepwise, newx = x_test)

summary(fit.stepwise)
```

## 6-6 Polynomial Regression (degree=2)
```{r}
x <- data_train[,-1]
fit.poly <- lm(position_numeric ~ poly(x[,1], degree=2, raw=TRUE)
                               + poly(x[,2], degree=2, raw=TRUE)
                               + poly(x[,3], degree=2, raw=TRUE)
                               + poly(x[,4], degree=2, raw=TRUE)
                               + poly(x[,5], degree=2, raw=TRUE)
                               + poly(x[,6], degree=2, raw=TRUE)
                               + poly(x[,7], degree=2, raw=TRUE)
                               + poly(x[,8], degree=2, raw=TRUE)
                               + poly(x[,9], degree=2, raw=TRUE)
                               + poly(x[,10], degree=2, raw=TRUE)
                               + poly(x[,11], degree=2, raw=TRUE)
                               + poly(x[,12], degree=2, raw=TRUE)
                               + poly(x[,13], degree=2, raw=TRUE)
                               + poly(x[,14], degree=2, raw=TRUE)
                               + poly(x[,15], degree=2, raw=TRUE)
                               + poly(x[,16], degree=2, raw=TRUE)
                               + poly(x[,17], degree=2, raw=TRUE)
                               + poly(x[,18], degree=2, raw=TRUE)
                               + poly(x[,19], degree=2, raw=TRUE)
                               + poly(x[,20], degree=2, raw=TRUE)
                               + poly(x[,21], degree=2, raw=TRUE)
                               + poly(x[,22], degree=2, raw=TRUE)
                               + poly(x[,23], degree=2, raw=TRUE)
                               + poly(x[,24], degree=2, raw=TRUE)
                               + poly(x[,25], degree=2, raw=TRUE)
                               + poly(x[,26], degree=2, raw=TRUE)
                               + poly(x[,27], degree=2, raw=TRUE)
                               + poly(x[,28], degree=2, raw=TRUE)
                               + poly(x[,29], degree=2, raw=TRUE)
                               + poly(x[,30], degree=2, raw=TRUE)
                               + poly(x[,31], degree=2, raw=TRUE)
                               + poly(x[,32], degree=2, raw=TRUE)
                               + poly(x[,33], degree=2, raw=TRUE)
                               + poly(x[,34], degree=2, raw=TRUE)
                               + poly(x[,35], degree=2, raw=TRUE), data=data_train)

poly.prediction <- predict(fit.poly, newx = x_test)

summary(fit.poly)
```

## 6-7 Polynomial Regression (degree=3)
```{r}
x <- data_train[,-1]
fit.poly2 <- lm(position_numeric ~ poly(x[,1], degree=3, raw=TRUE)
                               + poly(x[,2], degree=3, raw=TRUE)
                               + poly(x[,3], degree=3, raw=TRUE)
                               + poly(x[,4], degree=3, raw=TRUE)
                               + poly(x[,5], degree=3, raw=TRUE)
                               + poly(x[,6], degree=3, raw=TRUE)
                               + poly(x[,7], degree=3, raw=TRUE)
                               + poly(x[,8], degree=3, raw=TRUE)
                               + poly(x[,9], degree=3, raw=TRUE)
                               + poly(x[,10], degree=3, raw=TRUE)
                               + poly(x[,11], degree=3, raw=TRUE)
                               + poly(x[,12], degree=3, raw=TRUE)
                               + poly(x[,13], degree=3, raw=TRUE)
                               + poly(x[,14], degree=3, raw=TRUE)
                               + poly(x[,15], degree=3, raw=TRUE)
                               + poly(x[,16], degree=3, raw=TRUE)
                               + poly(x[,17], degree=3, raw=TRUE)
                               + poly(x[,18], degree=3, raw=TRUE)
                               + poly(x[,19], degree=3, raw=TRUE)
                               + poly(x[,20], degree=3, raw=TRUE)
                               + poly(x[,21], degree=3, raw=TRUE)
                               + poly(x[,22], degree=3, raw=TRUE)
                               + poly(x[,23], degree=3, raw=TRUE)
                               + poly(x[,24], degree=3, raw=TRUE)
                               + poly(x[,25], degree=3, raw=TRUE)
                               + poly(x[,26], degree=3, raw=TRUE)
                               + poly(x[,27], degree=3, raw=TRUE)
                               + poly(x[,28], degree=3, raw=TRUE)
                               + poly(x[,29], degree=3, raw=TRUE)
                               + poly(x[,30], degree=3, raw=TRUE)
                               + poly(x[,31], degree=3, raw=TRUE)
                               + poly(x[,32], degree=3, raw=TRUE)
                               + poly(x[,33], degree=3, raw=TRUE)
                               + poly(x[,34], degree=3, raw=TRUE)
                               + poly(x[,35], degree=3, raw=TRUE), data=data_train)

poly2.prediction <- predict(fit.poly2, newx = x_test)

summary(fit.poly2)
```

## 6-8 Polynomial Regression (degree=8)
```{r}
x <- data_train[,-1]
fit.poly3 <- lm(position_numeric ~ poly(x[,1], degree=8, raw=TRUE)
                               + poly(x[,2], degree=8, raw=TRUE)
                               + poly(x[,3], degree=8, raw=TRUE)
                               + poly(x[,4], degree=8, raw=TRUE)
                               + poly(x[,5], degree=8, raw=TRUE)
                               + poly(x[,6], degree=8, raw=TRUE)
                               + poly(x[,7], degree=8, raw=TRUE)
                               + poly(x[,8], degree=8, raw=TRUE)
                               + poly(x[,9], degree=8, raw=TRUE)
                               + poly(x[,10], degree=8, raw=TRUE)
                               + poly(x[,11], degree=8, raw=TRUE)
                               + poly(x[,12], degree=8, raw=TRUE)
                               + poly(x[,13], degree=8, raw=TRUE)
                               + poly(x[,14], degree=8, raw=TRUE)
                               + poly(x[,15], degree=8, raw=TRUE)
                               + poly(x[,16], degree=8, raw=TRUE)
                               + poly(x[,17], degree=8, raw=TRUE)
                               + poly(x[,18], degree=8, raw=TRUE)
                               + poly(x[,19], degree=8, raw=TRUE)
                               + poly(x[,20], degree=8, raw=TRUE)
                               + poly(x[,21], degree=8, raw=TRUE)
                               + poly(x[,22], degree=8, raw=TRUE)
                               + poly(x[,23], degree=8, raw=TRUE)
                               + poly(x[,24], degree=8, raw=TRUE)
                               + poly(x[,25], degree=8, raw=TRUE)
                               + poly(x[,26], degree=8, raw=TRUE)
                               + poly(x[,27], degree=8, raw=TRUE)
                               + poly(x[,28], degree=8, raw=TRUE)
                               + poly(x[,29], degree=8, raw=TRUE)
                               + poly(x[,30], degree=8, raw=TRUE)
                               + poly(x[,31], degree=8, raw=TRUE)
                               + poly(x[,32], degree=8, raw=TRUE)
                               + poly(x[,33], degree=8, raw=TRUE)
                               + poly(x[,34], degree=8, raw=TRUE)
                               + poly(x[,35], degree=8, raw=TRUE), data=data_train)

poly3.prediction <- predict(fit.poly3, newx = x_test)

summary(fit.poly3)
```

## 6-9 Lasso Regression
Implementation of the feature Selection via Lasso Regression
The number of independent variable : 35 -> 32
```{r}
library(glmnet)

set.seed(5555)
fit.lasso <- glmnet(x1, y1, alpha=1, family="gaussian")
fit.lasso.cv <- cv.glmnet(x1, y1, alpha=1, nfolds=10, type.measure="mse", family="gaussian")

lasso.coef = predict(fit.lasso, type = "coefficients", s=fit.lasso$lambda.min)
lasso.coef.cv = predict(fit.lasso.cv, type = "coefficients", s=fit.lasso.cv$lambda.min) # coefficients

lasso.prediction = predict(fit.lasso, s=fit.lasso$lambda.min, newx = x2) 
lasso.prediction.cv = predict(fit.lasso.cv, s=fit.lasso.cv$lambda.min, newx = x2) # coefficients

print(lasso.coef.cv)
 
# summary(fit.lasso)
summary(fit.lasso.cv)
```


## 6-10 Ridge Regression 
The number of independent variable : 35 -> 35
```{r}
# Ridge regression

set.seed(6666)
fit.ridge <- glmnet(x1, y1, alpha=0, family="gaussian")
fit.ridge.cv <- cv.glmnet(x1, y1, alpha=0, nfolds=10, type.measure="mse", family="gaussian")

ridge.coef = predict(fit.ridge, type = "coefficients", s=fit.ridge$lambda.min)
ridge.coef.cv = predict(fit.ridge.cv, type = "coefficients", s=fit.ridge.cv$lambda.min) # coefficients

ridge.prediction = predict(fit.ridge, s=fit.ridge$lambda.min, newx = x2)
ridge.prediction.cv = predict(fit.ridge.cv, s=fit.ridge.cv$lambda.min, newx = x2) # coefficients

print(ridge.coef.cv)
 
# summary(fit.ridge)
summary(fit.ridge.cv)
```




## 6-11 ElasticNet Regression 
The number of independent variable : 35 -> 28
```{r}
# ELASTIC NET WITH 0 < ALPHA < 1

set.seed(8888)

a3 <- seq(0.02, 0.98, 0.02)
search <- foreach(i = a3, .combine = rbind) %dopar% {
  cv <- cv.glmnet(x1, y1, alpha = i, nfold = 10, type.measure = "mse", family="gaussian")
  data.frame(cvm = cv$cvm[cv$lambda == cv$lambda.min], lambda.1se = cv$lambda.1se, alpha = i)
}


cv3 <- search[search$cvm == min(search$cvm), ]

fit.elnet <- glmnet(x1, y1, alpha = cv3$alpha, family="gaussian")
fit.elnet.cv <- cv.glmnet(x1, y1, alpha = cv3$alpha, nfolds=10, type.measure="mse", family="gaussian")

elnet.coef = predict(fit.elnet, type = "coefficients", s=fit.elnet$lambda.min)
elnet.coef.cv = predict(fit.elnet.cv, type = "coefficients", s=fit.elnet.cv$lambda.min) # coefficients

elnet.prediction = predict(fit.elnet, s=fit.elnet$lambda.min, newx = x2)
elnet.prediction.cv = predict(fit.elnet.cv, s=fit.elnet.cv$lambda.min, newx = x2) # coefficients

# coef(fit.elnet)
coef(fit.elnet.cv)
 
# summary(fit.elnet)
summary(fit.elnet.cv)
```


## 6-12 Model Performance Metrics for all models.

R-squared (R2), which is the proportion of variation in the outcome that is explained by the predictor variables. 
In multiple regression models, R2 corresponds to the squared correlation between the observed outcome values and the predicted values by the model. The Higher the R-squared, the better the model.

Root Mean Squared Error (RMSE), which measures the average error performed by the model in predicting the outcome for an observation. Mathematically, the RMSE is the square root of the mean squared error (MSE), which is the average squared difference between the observed actual outome values and the values predicted by the model. So, MSE = mean((observeds - predicteds)^2) and RMSE = sqrt(MSE). The lower the RMSE, the better the model.

Mean Absolute Error (MAE), like the RMSE, the MAE measures the prediction error. Mathematically, it is the average absolute difference between observed and predicted outcomes, MAE = mean(abs(observeds - predicteds)). MAE is less sensitive to outliers compared to RMSE.

```{r}
# MSE = mean((observeds - predicteds)^2)
# RMSE = sqrt(MSE)
# MAE = mean(abs(observeds - predicteds))

performance <- data.frame(
        MODEL_NAME = c("Linear", "Backward", "Forward", "Stepwise", "Polynomial(degree=2)"
                       , "Polynomial(degree=3)", "Polynomial(degree=8)", "Lasso", "Lidge", "ElasticNet")
        ,  
          
        RMSE = c(sqrt(mean((y_test - lm.prediction)^2)),
            sqrt(mean((y_test - bwd.prediction)^2)),
            sqrt(mean((y_test - fwd.prediction)^2)),
            sqrt(mean((y_test - stepwise.prediction)^2)),
            sqrt(mean((y_test - poly.prediction)^2)),
            sqrt(mean((y_test - poly2.prediction)^2)),
            sqrt(mean((y_test - poly3.prediction)^2)),
            sqrt(mean((y_test - lasso.prediction)^2)),
            sqrt(mean((y_test - ridge.prediction)^2)),
            sqrt(mean((y_test - elnet.prediction)^2)))
        ,
            
        MAE = c(mean(abs(y_test - lm.prediction)),
            mean(abs(y_test - bwd.prediction)),
            mean(abs(y_test - fwd.prediction)),
            mean(abs(y_test - stepwise.prediction)),
            mean(abs(y_test - poly.prediction)),
            mean(abs(y_test - poly2.prediction)),
            mean(abs(y_test - poly3.prediction)),
            mean(abs(y_test - lasso.prediction)),
            mean(abs(y_test - ridge.prediction)),
            mean(abs(y_test - elnet.prediction)))
        )

print(performance)

```
The result is that the Lasso Recession model has the smallest value in RMSE and MAE.



## 6-13 RMSE on test set with 10-Folds Cross Validation of Lasso, Lidge, ElasticNet.
```{r}
# RMSE on test set
estimation <- data.frame(
  regression = c("Lasso", "Lidge", "ElasticNet"),
  RMSE = c(sqrt(mean((y2 - lasso.prediction.cv)^2)), sqrt(mean((y2 - ridge.prediction.cv)^2)), 
          sqrt(mean((y2 - elnet.prediction.cv)^2))),
  MAE = c(mean(abs(y2 - lasso.prediction.cv)), mean(abs(y2 - ridge.prediction.cv)), 
          mean(abs(y2 - elnet.prediction.cv)))
  ) 
estimation

```
The result is that the ElasticNet Recession model has the smallest value in RMSE.



## 6-14 visualization of LASSO, Ridge, Elastic Net
Plot solution path and cross-validated MSE as function of ¥ë.
```{r}
# Plot solution paths:
par(mfrow=c(3,2))
# For plotting options, type '?plot.glmnet' in R console
plot(fit.lasso, xvar="lambda", main="LASSO")
plot(fit.lasso.cv, main="LASSO")

plot(fit.ridge, xvar="lambda", main="Ridge")
plot(fit.ridge.cv, main="Ridge")

plot(fit.elnet, xvar="lambda", main="Elastic Net")
plot(fit.elnet.cv, main="Elastic Net")
```

## 6-15 Visualization of Lasso and ElasticNet Regresstion Model
```{r}
par(mfrow=c(1,2))

# Lasso predicted and observed
plot(y2, lasso.prediction.cv, ylim=c(min(lasso.prediction.cv), max(lasso.prediction.cv)), xlim=c(min(y2), max(y2)),main="Test Dataset", xlab="observed", ylab="lasso Predicted")
abline(0, 1, col="red")

# ElasticNet predicted and observed
plot(y2, elnet.prediction.cv, ylim=c(min(elnet.prediction.cv), max(elnet.prediction.cv)), xlim=c(min(y2), max(y2)),main="Test Dataset", xlab="observed", ylab="ElasticNet Predicted")
abline(0, 1, col="red")

```



## 6-16 Apply my soccer score with all regression models.
recommend my soccer position with all regression models.
```{r}
# prepare my dataset for recommendation of my soocer position. 

# create dataset
me_test <- x_test

# input my soccer score
me_test$height[1] <- 173
me_test$weight[1] <- 171
me_test$preferred_foot[1] <- 2
me_test$crossing[1] <- 80
me_test$finishing[1] <- 85
me_test$heading_accuracy[1] <- 60
me_test$short_passing[1] <- 90
me_test$volleys[1] <- 75
me_test$dribbling[1] <- 95
me_test$curve[1] <- 85
me_test$free_kick_accuracy[1] <- 80
me_test$long_passing[1] <- 90
me_test$ball_control[1] <- 95
me_test$acceleration[1] <- 50
me_test$sprint_speed[1] <- 30
me_test$agility[1] <- 70
me_test$reactions[1] <- 80
me_test$balance[1] <- 75
me_test$shot_power[1] <- 85
me_test$jumping[1] <- 40
me_test$stamina[1] <- 40
me_test$strength[1] <- 75
me_test$long_shots[1] <- 85
me_test$aggression[1] <- 90
me_test$interceptions[1] <- 65
me_test$positioning[1] <- 80
me_test$vision[1] <- 95
me_test$marking[1] <- 60
me_test$standing_tackle[1] <- 30
me_test$sliding_tackle[1] <- 25
me_test$gk_diving[1] <- 7
me_test$gk_handling[1] <- 7
me_test$gk_kicking[1] <- 12
me_test$gk_positioning[1] <- 7
me_test$gk_reflexes[1] <- 7

# convert dataframe to matrix.
me_test_matrix <- as.matrix(me_test)


# recommendation of my soccer position with all regresstion models.

set.seed(99999)

lm.prediction.me <- predict(fit.lm, newx = me_test_matrix)
bwd.prediction.me <- predict(fit.bwd, newx = me_test_matrix)
fwd.prediction.me <- predict(fit.fwd, newx = me_test_matrix)
stepwise.prediction.me <- predict(fit.stepwise, newx = me_test_matrix)
poly.prediction.me <- predict(fit.poly, newx = me_test_matrix)
poly2.prediction.me <- predict(fit.poly2, newx = me_test_matrix)
poly3.prediction.me <- predict(fit.poly3, newx = me_test_matrix)
lasso.prediction.me <- predict(fit.lasso, s=fit.lasso$lambda.min, newx = me_test_matrix) 
lasso.prediction.cv.me <- predict(fit.lasso.cv, s=fit.lasso.cv$lambda.min, newx = me_test_matrix)
ridge.prediction.me <- predict(fit.ridge, s=fit.ridge$lambda.min, newx = me_test_matrix)
ridge.prediction.cv.me <- predict(fit.ridge.cv, s=fit.ridge.cv$lambda.min, newx = me_test_matrix)
elnet.prediction.me <- predict(fit.elnet, s=fit.elnet$lambda.min, newx = me_test_matrix)
elnet.prediction.cv.me <- predict(fit.elnet.cv, s=fit.elnet.cv$lambda.min, newx = me_test_matrix)



# result of recommendation my soccer position with all regresstion models.

recom_myposition <- data.frame(
        Linear <- lm.prediction.me[1], 
        Backward <- bwd.prediction.me[1],
        Forward <- fwd.prediction.me[1], 
        Stepwise <- stepwise.prediction.me[1],
        Polynomial1 <- poly.prediction.me[1],
        Polynomial2 <- poly2.prediction.me[1],
        Polynomial3 <- poly3.prediction.me[1], 
        Lasso <- poly3.prediction.me[1], 
        Lasso.cv <- lasso.prediction.me[1],
        Lasso  <- lasso.prediction.cv.me[1], 
        Lidge  <- ridge.prediction.me[1], 
        Lidge.cv  <- ridge.prediction.cv.me[1], 
        ElasticNet  <- elnet.prediction.me[1], 
        ElasticNet.cv <- elnet.prediction.cv.me[1])


# create dataframe of result.
recom_myposition <- data.frame(
          c("Linear", "Backward", "Forward", "Stepwise", "Polynomial1",
            "Polynomial2", "Polynomial3", "Lasso", "Lasso.cv", "Lidge", "Lidge.cv",
            "ElasticNet", "ElasticNet.cv"),
  
           c(lm.prediction.me[1], 
            bwd.prediction.me[1],
            fwd.prediction.me[1], 
            stepwise.prediction.me[1],
            poly.prediction.me[1],
            poly2.prediction.me[1],
            poly3.prediction.me[1], 
            lasso.prediction.me[1],
            lasso.prediction.cv.me[1], 
            ridge.prediction.me[1], 
            ridge.prediction.cv.me[1], 
            elnet.prediction.me[1], 
            elnet.prediction.cv.me[1]))

colnames(recom_myposition) <- c("Regression", "My position recommend")

recom_myposition
 
```
I am satisfied with the results of Lasso and Lidge and ElasticNet regression models.
Among them, I was most satisfied with the result of using ElasticNet.cv model.
Because it is my preferred soccer position.
So I will choose a ElasticNet regression model using 10-Folds cv.



# 7. Summary
First, we preprocessed the dataset with removed missing value, unnecessary variables, convering variable type.
And then, we checked linear model assumptions such as Normality, Independence, Heteroscedasticity, Multicolinearity.
The dataset does not meet the assumption of a linear model and it also has multicollinearity problems. 
In order to increase performance, we used some regresstion models such as Backward Regression, Forward Regression,
Stepwise Regression, Polynomial Regression, Lasso Regression, Ridge Regression, ElasticNet Regression.
And we evaluated all regression models with RMSE, MAE. 
The result is that the Lasso, ElasticNet Recession models has the smallest value in RMSE and MAE.
Finally, I input my soccer position information to all regression models for predicton of my position.
The results were satisfied only on some models.
Among them, I was most satisfied with the result of using ElasticNet.cv model.
Because it matches my preferred soccer position,



# 8. Conclusion
In order to provide soccer position recommend service to the clients,
We will choose a ElasticNet regression model with 10-Folds cv.

Because it has the best performance among regression models. 
And the elastic net model is that it enables effective regularization 
via the ridge penalty with the feature selection characteristics of the lasso penalty. 
Effectively, elastic nets allow us to control multicollinearity concerns, 
perform regression when p>n, and reduce excessive noise in our data so that
we can isolate the most influential variables while balancing prediction accuracy.

However, elastic nets, and regularization models in general, still assume linear relationships 
between the features and the target variable. 
And although we can incorporate non-additive models with interactions, 
doing this when the number of features is large is extremely tedious and difficult. 
When non-linear relationships exist, its beneficial to start exploring non-linear regression approaches.



# 9. Suggestion for our Clients
Our clients are all people who love soccer. If you want to know your soccer position for your soccer match.
You can use our service that recommend soccer position if you input your information data to our service.
This service has machine learning technology using ElasticNet regression model. 
And also this service has an average error of 4.651255(MAE). 
Is there a service with better performance? Choose our sevice. You will not regret it.















# =======================================
date: "2019³â 6¿ù 2ÀÏ"
# Assingment #3 (Time Series)


# 1. Objectives(Business problem)
The objective of this project is to forecast the number of natinoal soccer match in next three years. 
Using time series models, we create a service that we offer information which is the number of natinoal soccer match in the near future.

# 2. Hypothesis
the monthly national soccer matches can be predicted through time series analysis with a prepared dataset.

# 3. Select Optimal Dataset
## Dataset Description (single variable for time series analytics)
The dataset includes 1,416 instances and 1 response variable and 1 time variable.
This dataset contains data for the number of national soccer match monthly in the past over 100 years (1900 ~ 2017).
The data is sourced from https://www.kaggle.com/martj42/international-football-results-from-1872-to-2017 website and 
contains various data such as below

### 1) time variable (1900 ~ 2017, month)
### 2) int variable (the number of national soccer match)


# 4. Data Preparation
cleaning and transforming raw data prior to processing and analysis
```{r}
library(AER)
library(readxl)
library(dynlm)
library(quantmod)
library(scales)
library(fGarch)
```

## 4-1 Load a dataset.
```{r}
# load the dataset
library(readxl)
match = read_xls("C://Users//joon//Documents//soccer_national_match05.xls")
# head(position.data)
str(match)
dim(match)
```

## 4-2 Checking Missing Value
result : no null values.
```{r}
table(is.na.data.frame(match))
```

## 4-3 Converting the dataset to a Time series object
```{r}
match.ts <- ts(match$num_match, start = c(1900, 1),end = c(2017, 12),frequency = 12)
```


# 5. Exploratory Data Analysis

## 5-1 Plotting the time series data.
result : It is a time series graph that has trend and seasonal components and increases variation with time.
Ãß¼¼¼ººÐ°ú °èÀý¼ººÐÀ» °®°í ½Ã°£ÀÇ º¯È­¿¡ µû¶ó º¯µ¿ÆøÀÌ Ä¿Áö´Â ½Ã°è¿­ ±×·¡ÇÁ ÇüÅÂÀÌ´Ù.
```{r}
plot(match.ts)  
abline(reg = lm(match.ts~time(match.ts)))
```

## 5-2 Checking for Stationarity
Stationarity : At any point(time), the average, variance, and auto-covariance measurements have the same value.

Á¤»ó¼º Á¶°Ç : ¾î¶² ½ÃÁ¡¿¡¼­ Æò±Õ, ºÐ»ê, ÀÚ±â°øºÐ»êÀ» ÃøÁ¤ÇÏ´õ¶óµµ µ¿ÀÏÇÑ °ªÀ» °®´Â´Ù.

(1) Æò±ÕÀÌ ÀÏÁ¤ : ¸ðµç ½ÃÁ¡¿¡ ´ëÇØ ÀÏÁ¤ÇÑ Æò±ÕÀ» °¡Áø´Ù.
- Æò±ÕÀÌ ÀÏÁ¤ÇÏÁö ¾ÊÀº ½Ã°è¿­Àº Â÷ºÐ(difference)À» ÅëÇØ Á¤»óÈ­
- Â÷ºÐÀº Çö½ÃÁ¡ ÀÚ·á¿¡¼­ ÀÌÀü ½ÃÁ¡ ÀÚ·á¸¦ »©´Â °Í
(2) ºÐ»êµµ ½ÃÁ¡¿¡ ÀÇÁ¸ÇÏÁö ¾ÊÀ½
- ºÐ»êÀÌ ÀÏÁ¤ÇÏÁö ¾ÊÀº ½Ã°è¿­Àº º¯È¯(transformation)À» ÅëÇØ Á¤»óÈ­
(3) °øºÐ»êµµ ½ÃÂ÷¿¡¸¸ ÀÇÁ¸ÇÒ »Ó, Æ¯Á¤ ½ÃÁ¡¿¡´Â ÀÇÁ¸ÇÏÁö ¾ÊÀ½

result : From the result we can clearly see that P value < 0.05 which means that we accept the 
alternative hypothesis,i.e., the given Time series is stationary.
```{r}
library(tseries)
library(forecast)
adf.test(match.ts, alternative = "stationary")
```

## 5-3 Extracting the Seasonality and Trend
result : The below graph shows that the mean and the variance are not constant.
°á°ú : trend±×·¡ÇÁ¸¦ º¸¸é Æò±ÕÀÌ ÀÏÁ¤ÇÏÁö ¾Ê°í observed±×·¡ÇÁ¸¦ º¸¸é ºÐ»êÀÌ Á¶±Ý¾¿ Áõ°¡ÇÏ´Â °ÍÀ» ¾Ë ¼ö ÀÖ´Ù.
```{r}
x<-decompose(match.ts,type = c("multiplicative"))
plot(x)
# Through this plot we can easily identify the Trend, Seasonality and Irregular
# components in our Time series
```

## 5-4 Seasonal displays
°á°ú : 6¿ùÀ» Á¦¿ÜÇÏ°í´Â °èÀýº° °æ±â¼ö´Â Å« Â÷ÀÌ°¡ ¾ø´Ù. 

result : Except for June, there is no big difference in the number of soccer match per season.
```{r}
monthplot(match.ts)
```


# 6. Select of Optimal Models and Visualization.

## 6-1 ARIMA Model

### 6-1-1 Differencing for converting Non-stationary to stationary.
Since our series is Non-stationary, we have to make it stationary. 

result : d = 1
```{r}
# tells us the number of differences(d) required to achieve stationarity
ndiffs(match.ts)
```

°á°ú : 1Â÷ Â÷ºÐ¸¸ ÇØµµ ¾î´ÀÁ¤µµ Á¤»óÈ­ ÆÐÅÏÀ» º¸ÀÓ. 
```{r}
# Â÷ºÐÀ» ÅëÇØ µ¥ÀÌÅÍ Á¤»óÈ­
match_diff1 <- diff(match.ts, differences = 1)
match_diff2 <- diff(match.ts, differences = 2)
match_diff3 <- diff(match.ts, differences = 3)

par(mfrow = c(2,2))

plot.ts(match.ts)
plot.ts(match_diff1)    # 1Â÷ Â÷ºÐ¸¸ ÇØµµ ¾î´ÀÁ¤µµ Á¤»óÈ­ ÆÐÅÏÀ» º¸ÀÓ
plot.ts(match_diff2)
plot.ts(match_diff3)
```

### 6-1-2 Trend of increasing variance -> adjusting variance by log transformation
result : Even after log transformation, variances differ at certain times.
So we will not use log transformation.
```{r}
#just orginal time series data
plot.ts(match.ts)

#converting with log
match.ts_log <- log(match.ts)
plot.ts(match.ts_log)
```


### 6-1-3 Determination of Order of Auto regression(p) and order of Moving averages(q)
now we know the value of p,d and q we can create our model using Arima(p,d,q) function

Acf()·Î lag ¸î¿¡¼­ Àý´Ü°ªÀÎÁö ¾Ë ¼ö ÀÖ´Ù. ¿¹¸¦ µé¾î lag 1 Àý´Ü°ª ÀÌ¸é MA(0)°¡ µÇ°í q=0 °¡ µÈ´Ù.
°á°ú : Â÷ºÐ ÀÌÈÄ¿¡µµ Àý´Ü°ªÀÌ ¸íÈ®ÇÏÁö ¾Ê¾Æ¼­ ARIMA ¸ðÇü È®Á¤ÀÌ ¾î·Æ´Ù.
```{r}
par(mfrow = c(1,2))

# Autocorrelation function
# This is used for finding the order of Auto regression(p)
acf(match.ts)

# This is used for finding the order of Auto regression(p) with d=1 series.
acf(match_diff1)     
```


Pacf()·Î lag ¸î¿¡¼­ Àý´Ü°ªÀÎÁö ¾Ë ¼ö ÀÖ´Ù. ¿¹¸¦ µé¾î lag 4 Àý´Ü°ª ÀÌ¸é AR(3)°¡ µÇ°í p=3 °¡ µÈ´Ù.
°á°ú : Â÷ºÐ(d=1) ÀÌÈÄ¿¡µµ Àý´Ü°ªÀÌ ¸íÈ®ÇÏÁö ¾Ê¾Æ¼­ ARIMA ¸ðÇü È®Á¤ÀÌ ¾î·Æ´Ù.
```{r}
par(mfrow = c(1,2))

# Partial Autocorrelation function
# This is used for finding the order of Moving averages(q)
pacf(match.ts) 

# This is used for finding the order of Moving averages(q) with d=1 series.
pacf(match_diff1)    
```

Acf() and Pacf() in forecast, along with a combination display using tsdisplay()

°á°ú : tsdisplay()¸¦ »ç¿ëÇÑ ACF, PACF¸¦ ÅëÇØ¼­µµ Àý´Ü°ªÀÌ ¸íÈ®ÇÏÁö ¾Ê¾Æ¼­ ARIMA ¸ðÇü È®Á¤ÀÌ ¾î·Æ´Ù.
```{r}
tsdisplay(match_diff1)
```

### 6-1-4 Using the Auto.Arima function with match.ts
```{r}
match_auto.arima <- auto.arima(match.ts)
summary(match_auto.arima)
accuracy(match_auto.arima)  #  Accuracy of the Model
```

### 6-1-5 Forecasting the future values
Forecasts 2018 ~ 2020 from ARIMA(1,0,1)(0,1,2)[12]
```{r}
match_fcast <- forecast(match_auto.arima, h=36, level = 95)
plot(match_fcast, main="Forecasts 2018 ~ 2020 from ARIMA(1,0,1)(0,1,2)[12]")
```

### 6-1-6 Auto.Arima Model Evaluation
The residuals are even.
But p-value(lag = 1,2,3) can not reject null hypothesis(= auto-correlation is zero)

ÀÜÂ÷°¡ ±ÕµîÇÏ´Ù.
ÇÏÁö¸¸ p-value°¡ lag=1,2,3 ¿¡¼­¸¸ ±Í¹«°¡¼³(ÀÚ±â»ó°üÀÌ 0ÀÌ´Ù.)À» ±â°¢ÇÏÁö ¸øÇÑ´Ù.
°á°úÀûÀ¸·Î ¿ì¸®´Â ÀÏÁ¤ ºÎºÐ¿¡¼­ ÀûÀýÇÏÁö ¾ÊÀº Arima ¸ðÇüÀ» ¼±Á¤ÇÑ °ÍÀÌ´Ù.
±×·¡¼­ LSTM ¸ðµ¨À» Àû¿ëÇØ º¸µµ·Ï ÇÏ°Ú´Ù.
```{r}
tsdiag(match_auto.arima)
```


## 6-2 LSTM (Long Short-Term Memory Units)
RNNÀº °ü·Ã Á¤º¸¿Í ±× Á¤º¸¸¦ »ç¿ëÇÏ´Â ÁöÁ¡ »çÀÌ °Å¸®°¡ ¸Ö °æ¿ì ¿ªÀüÆÄ½Ã ±×·¡µð¾ðÆ®°¡ Á¡Â÷ ÁÙ¾î ÇÐ½À´É·ÂÀÌ Å©°Ô ÀúÇÏµÇ´Â °ÍÀ¸·Î ¾Ë·ÁÁ® ÀÖ½À´Ï´Ù. 
ÀÌ ¹®Á¦¸¦ ±Øº¹ÇÏ±â À§ÇØ¼­ °í¾ÈµÈ °ÍÀÌ ¹Ù·Î LSTMÀÔ´Ï´Ù. LSTMÀº RNNÀÇ È÷µç state¿¡ cell-state¸¦ Ãß°¡ÇÑ ±¸Á¶ÀÔ´Ï´Ù. 
LSTMÀº ¿ÀÂ÷ÀÇ ±×¶óµð¾ðÆ®°¡ ½Ã°£À» °Å½½·¯¼­ Àß Èê·¯°¥ ¼ö ÀÖµµ·Ï µµ¿ÍÁÝ´Ï´Ù. 
backpropÇÏ´Â °úÁ¤¿¡¼­ ¿ÀÂ÷ÀÇ °ªÀÌ ´õ Àß À¯ÁöµÇ´Âµ¥, °á°úÀûÀ¸·Î 1000´Ü°è°¡ ³Ñ°Ô °Å½½·¯ ¿Ã¶ó°¥ ¼ö ÀÖ½À´Ï´Ù.

### 6-2-1 Load the neccessary libraries & the dataset
```{r}
#install.packages("devtools")
#devtools::install_github("rstudio/keras")
#install_tensorflow() # ÀÏ¹ÝÀûÀÎ 
#install_keras(tensorflow = "gpu") #³ëÆ®ºÏÀÌ GPU Áö¿øÇÏ´Â °æ¿ì
#install_keras() #³ëÆ®ºÏÀÌ ÀÏ¹Ý ³»ÀåÇü ±×·¡ÇÈÄ«µåÀÏ °æ¿ì

library(devtools)
library(tensorflow)
library(keras)
```

### 6-2-2 Data preparation
1) Transform data to stationary
```{r}
# transform data to stationarity
diffed = diff(match.ts, differences = 1)
head(diffed)
```

2) Lagged dataset
LSTM expects the data to be in a supervised learning mode. That is, having a target variable Y and predictor X. To achieve this, we transform the series by lagging the series and have the value at time (t-k) as the input and value at time t as the ouput, for a k-step lagged dataset. 
```{r}
# create a lagged dataset, i.e to be supervised learning
lag_transform <- function(x, k= 1){
    
      lagged =  c(rep(NA, k), x[1:(length(x)-k)])
      DF = as.data.frame(cbind(lagged, x))
      colnames(DF) <- c( paste0('x-', k), 'x')
      DF[is.na(DF)] <- 0
      return(DF)
}
supervised = lag_transform(diffed, 1)
head(supervised)
```

3) Split dataset into training and testing sets
Unlike in most analysis where training and testing data sets are randomly sampled, with time series data the order of the observations does matter. The following code split the first 70% of the series as training set and the remaining 30% as test set.
```{r}
## split into train and test sets

N = nrow(supervised)
n = round(N *0.85, digits = 0)
train = supervised[1:n, ]
test  = supervised[(n+1):N,  ]
```


4) Normalize the data
Just like in any other neural network model, we rescale the input data X to the range of the activation function. As shown earlier, the default activation function for LSTM is sigmoid function whose range is [-1, 1]. The code below will help in this transformation. Note that the min and max values of the training data set are the scaling coefficients used to scale both the training and testing data sets as well as the predicted values. This ensures that the min and max values of the test data do not influence the model.
```{r}
## scale data
scale_data = function(train, test, feature_range = c(0, 1)) {
  x = train
  fr_min = feature_range[1]
  fr_max = feature_range[2]
  std_train = ((x - min(x) ) / (max(x) - min(x)  ))
  std_test  = ((test - min(x) ) / (max(x) - min(x)  ))
  
  scaled_train = std_train *(fr_max -fr_min) + fr_min
  scaled_test = std_test *(fr_max -fr_min) + fr_min
  
  return( list(scaled_train = as.vector(scaled_train), scaled_test = as.vector(scaled_test) ,scaler= c(min =min(x), max = max(x))) )
  
}


Scaled = scale_data(train, test, c(-1, 1))

y_train = Scaled$scaled_train[, 2]
x_train = Scaled$scaled_train[, 1]

y_test = Scaled$scaled_test[, 2]
x_test = Scaled$scaled_test[, 1]
```

The following code will be required to revert the predicted values to the original scale.
```{r}
## inverse-transform
invert_scaling = function(scaled, scaler, feature_range = c(0, 1)){
  min = scaler[1]
  max = scaler[2]
  t = length(scaled)
  mins = feature_range[1]
  maxs = feature_range[2]
  inverted_dfs = numeric(t)
  
  for( i in 1:t){
    X = (scaled[i]- mins)/(maxs - mins)
    rawValues = X *(max - min) + min
    inverted_dfs[i] <- rawValues
  }
  return(inverted_dfs)
}

```


### 6-2-3 Modeling
```{r relu, echo=FALSE, fig.cap="A caption", out.width = '100%'}
knitr::include_graphics("C://Users//joon//Documents//gates_lstm.png")
```

1) Define the model
È÷µç ·¹ÀÌ¾î¿¡¼­ ³Ñ°Ü¹ÞÀº »óÅÂ¸¦ ´ÙÀ½ ¹èÄ¡ÀÇ »ùÇÃ¿¡ ´ëÇÑ ÃÊ±â »óÅÂ·Î Àç»ç¿ëÇÒ ¼ö ÀÖµµ·Ï stateful= TRUE·Î ¼³Á¤ÇÏ°í, ³×Æ®¿öÅ©°¡ »óÅÂ ÀúÀåÀûÀÌ±â ¶§¹®¿¡ ÇöÀçÀÇ [samples, features]·Î ºÎÅÍ 3Â÷¿ø ¹è¿­ÀÇ [samples, timesteps, features]·Î ÀÔ·Â ¹èÄ¡·Î ¸¸µé¾î¾ß ÇÑ´Ù.

(1) Samples: Number of observations in each batch. In this model the batch size = 1.

(2) Timesteps: Separate time steps for a given observations. In this model the timesteps = 1

(3) Features: For a univariate case, like in this model, the features = 1

```{r}
# Reshape the input to 3-dim
dim(x_train) <- c(length(x_train), 1, 1)

# specify required arguments
X_shape2 = dim(x_train)[2]
X_shape3 = dim(x_train)[3]
batch_size = 1                # must be a common factor of both the train and test samples
units = 1                     # can adjust this, in model tuninig phase

#=========================================================================================

model <- keras_model_sequential() 
model%>%
  layer_lstm(units, batch_input_shape = c(batch_size, X_shape2, X_shape3), stateful= TRUE)%>%
  layer_dense(units = 1)
```


2) Compile the model (model constructure)
Here I have specified the mean_squared_error as the loss function, Adaptive Monument Estimation (ADAM) as the optimization algorithm and learning rate and learning rate decay over each update. Finaly, I have used the accuracy as the metric to assess the model performance.
```{r}
model %>% compile(
  loss = 'mean_squared_error',
  optimizer = optimizer_adam( lr= 0.02, decay = 1e-6 ),  
  metrics = c('accuracy')
)

summary(model)
```


3) Fit the model (Traing)
We set the argument shuffle = FALSE to avoid shuffling the training set and maintain the dependencies between xi and xi+t. LSTM also requires resetting of the network state after each epoch. To achive this we run a loop over epochs where within each epoch we fit the model and reset the states via the argument reset_states().
```{r}
Epochs = 100   
for(i in 1:Epochs ){
  model %>% fit(x_train, y_train, epochs=1, batch_size=batch_size, verbose=1, shuffle=FALSE)
  model %>% reset_states()
}
```

4) Make predictions
```{r}
L = length(x_test)
scaler = Scaled$scaler
predictions = numeric(L)

for(i in 1:L){
     X = x_test[i]
     dim(X) = c(1,1,1)
     yhat = model %>% predict(X, batch_size=batch_size)
     # invert scaling
     yhat = invert_scaling(yhat, scaler,  c(-1, 1))
     # invert differencing
     yhat  = yhat + match.ts[(n+i)]
     # store
     predictions[i] <- yhat
}
```


### 6-2-4 Plot to check accuracy of predicted value. 

```{r}
#prepare real value
real_y_value <- match$num_match

#prepare predicted value
x <- c()
x[1:1204] <- "NA" 
predictions <- c(x, predictions)

#plot predicted value(train:test=0.85:0.15) & real value
plot(x=1:length(real_y_value), y=real_y_value, type="l", col="gray", xlab="time index", ylab="the number of match, monthly")
lines(x=1:length(real_y_value), y=predictions, col="red")
title(main="predicted value(train:test=0.85:0.15) & real value")
legend("topleft", c("predicted value(red)", "real value(gray)"), cex=1.2)
```


# 7. Summary
First, Converted the dataset into Time series object using ts function.
And then, Checked for stationarity and plotted the acf and pacf plots.
And We Used decompose function to extract the seasonality and Trend components from the Time series.
In order to forecast, we applied auto.arima model but it was not suitable. 
because it is that p-value(lag = 1,2,3) can not reject null hypothesis(= auto-correlation is zero).
In order to increase performance, we applied LSTM model. 
We split train, test dataset with 0.85. and predicted value with test data.
We used Sigmoid to reduce Gradient Loss.
Finally, We plot to check accuracy of predicted value. It was just satisfied with plotting result. 


# 8. Conclusion
In order to offer better information to the clients, we selected LSTM model.
between ARIMA and LSTM, we were satisfied with the result of LSTM model.
I'm satisfied with the high accuracy, but I'm afraid of overfitting.
In the future, we will study ways to reduce LSTM's overfitting and get better results.


# 9. Recommendation for virtual clients & Further Development Direction
Our clients are all people who love soccer. If you want to know the number of national soccer match in the near future.
You should choose our service. This service has machine learning technology using LSTM model. 
Is there a service with better performance? Choose our sevice. You will not regret it.
