# =======================================
written by ideajoon
date: "2019-5-12"
# Assingment #2 (Feature Selection Regularizer)


# 1. Objectives(Business problem)
Using a regression model, we create a service that recommends soccer positions to clients.

# 2. Hypothesis
The soccer position can be predicted With position dataset of European soccer players in the last ten years.

# 3. Select Optimal Dataset
## 3-1. Dataset Description
The dataset includes 4184 instances and 1 response variable and 40 independent variables.
This dataset contains data for soccer position information of European 4,184 soccer players in the last 10 years.
The data is sourced from https://www.kaggle.com/hugomathien/soccer website and 
contains various data such as below

1)position_numeric 포지션을 수치로 표현
2)position_description 포지션 상세 설명
3)position_name 포지션 약어 명칭
4)height 키
5)weight 몸무기lb
6)overall_rating 전체
7)potential 잠재능력
8)preferred_foot 왼발잡이? 오른발잡이?
9)crossing 볼 크로스 능력
10)finishing 마무리 능력
11)heading_accuracy
12)short_passing
13)volleys 발리킥 능력
14)dribbling
15)curve 회전 킥 능력
16)free_kick_accuracy 프리킥 정확도
17)long_passing
18)ball_control
19)acceleration 질주 가속도
20)sprint_speed 질주 속도
21)agility 민첩성
22)reactions 반응 능력
23)balance 넘어지지 않는
24)shot_power
25)jumping 점프능력
26)stamina 체력
27)strength 힘
28)long_shots 롱슛 능력
29)aggression 공격성
30)interceptions 볼 가로채기 능력
31)positioning 위치 선정 능력
32)vision 시야
33)penalties 페널트킥 성공률
34)marking 마킹 능력
35)standing_tackle
36)sliding_tackle
37)gk_diving 키퍼 다이빙 능력
38)gk_handling 키퍼 공터치감
39)gk_kicking 키퍼 킥력
40)gk_positioning 키퍼 위치선정력
41)gk_reflexes 키퍼 반응력


## 3-2. Numeric Position's graph
```{r relu, echo=FALSE, fig.cap="A caption", out.width = '100%'}
knitr::include_graphics("C://Users//joon//Documents//numeric_soccer_position.png")
```


# 4. Data preprocessing

## 4-1 LOAD A DATASET.
```{r}
# load the dataset
getwd()
position.data <- read.csv(file="C://Users//joon//Documents//soccer_player_positions - v6.csv", header = TRUE, fileEncoding = "euc-kr")
# head(position.data)
str(position.data)
```


## 4-2 Remove Missing Value
```{r}
table(is.na.data.frame(position.data))
library(dplyr) # for removing missing value
position1.data <- na.omit(position.data) #Extracts missing data from all variables
table(is.na(position1.data))
```


## 4-3 Remove unnecessary categorical variables. 
```{r}
# relationship between position_name and numeric
rel_position <- position1.data[,c(1:3)] 
# Remove categorical variables and meaningless variables.
position1.data <- position1.data[,-c(2,3,6,7,33)]
# Removed categorical variables
# 2)position_description 포지션 상세 설명
# 3)position_name 포지션 약어 명칭
# Removed meaningless variables
# 6)overall_rating 전체
# 7)potential 잠재능력
# 33)penalties 페널트킥 성공률
```


## 4-4 Convert factor to numeric variable
```{r}
# convert factor(preferred_foot) to numeric variable 
# left_foot = 1, right_foot = 2
position1.data[,4] <- as.numeric(position1.data[,4])
```


# 5. Exploratory Data Analysis

## 5-1 linear regression model
```{r}
require(leaps)
# linear regression model
fit.lm <- lm(position_numeric~.,data=position1.data)
summary(fit.lm)
```
The null hypothesis: The regression model is not valid.
The alternative hypothesis: The regression model is valid.
Result : Since the p value is 0.0000, a rare model is reasonable at the significance level of 0.05.


## 5-2 Normality check
- null hypothesis: Follow the normal distribution.
- alternative hypothesis: It does not follow the normal distribution.
At a significant level of 0.05 p, the regression model is not normally distributed.
```{r}
library(car)
shapiro.test(fit.lm$residuals)
```

## 5-3 Independence check
- null hypothesis : Each error is independent.
- alternative hypothesis : Each error is not independent.
If the D-W statistic is close to 2, it is not self-correlated (each independent), or if it is further away from 2, 
it is self-correlated (each is not independent).
p-values do not meet the independence assumption of the regression model at a significant level of 0.05.
```{r}
car::durbinWatsonTest(fit.lm)
```

## 5-4 Heteroscedasticity check
- null hypothesis : Each error is Homogeneity of variance.
- alternative hypothesis : Each error is not Homogeneity of variance.(it is Heteroscedasticity of variance)
p값이 유의수준 0.05에서 희귀모형은 에러들은 등분산성을 만족하지 않는다.
```{r}
car::ncvTest(fit.lm)
```

## 5-5 Overall verification of linear model assumptions
- Global Stat : Overall satisfaction with errors, assumtions acceptable : generally satisfied
- Skewness(비대칭도), Kurtosis(첨도) : Normality test
- Link Funtion : Linearity test
- Heteroscedasticity : Heteroscedasticity test
At a significant level of 0.05 p of Global Stat, the regression model does not meet the assumption of a linear model.
```{r}
library(gvlma)
gvmodel<-gvlma(fit.lm)
summary(gvmodel)
fit.lm_glovalstat_pvalue <- gvmodel$GlobalTest$GlobalStat4$pvalue
```

## 5-6 Multicolinearity check
```{r}
sqrt(vif(fit.lm)) > 2
```


## 5-7 correlation matrix plot
A default correlation matrix plot (called a Correlogram) is generated. 
Positive correlations are displayed in a blue scale while negative correlations are displayed in a red scale.
```{r}
# scatter-plot matrix, correlation, histogram
library(psych)
pairs.panels(position1.data[,c(2:8)])
pairs.panels(position1.data[,c(9:15)])
pairs.panels(position1.data[,c(16:22)])
pairs.panels(position1.data[,c(23:29)])
pairs.panels(position1.data[,c(30:36)])
```


# 6. Select of Optimal Models and Visualization.
Apply dataset to some regression models and select of Optimal Models and Visualization of Model.

## 6-1 Create Training and Test Samples
```{r}
library(caret)
set.seed(1222)
split=0.70
trainIndex <- createDataPartition(position1.data$position_numeric, p=split, list=FALSE)
data_train <- position1.data[trainIndex, ]
data_test <- position1.data[-trainIndex, ]
x_train <- data_train[,-1]
y_train <- data_train[,1]
x_test <- data_test[,-1]
y_test <- data_test[,1]
x1 <- as.matrix(x_train)
y1 <- y_train
x2 <- as.matrix(x_test)
y2 <- y_test
```

## 6-2 Linear Regression
```{r}
# Linear Regresstion for train dataset
fit.lm <- lm(position_numeric~.,data=data_train)
lm.prediction <- predict(fit.lm, newx = x_test)
summary(fit.lm)
```


## 6-3 Backward Regression
The number of independent variable : 35 -> 26
```{r}
# starting from full model
full.model <- lm(position_numeric~.,data=data_train)
fit.bwd <- step(full.model,direction="backward", trace=0)
bwd.prediction <- predict(fit.bwd, newx = x_test)
summary(fit.bwd)
```

## 6-4 Forward Regression
The number of independent variable : 35 -> 26
```{r}
# starting from null model
min.model <- lm(position_numeric ~ 1, data=data_train)
biggest <- formula(lm(position_numeric~., position1.data))
fit.fwd <- step(min.model, direction="forward", scope=biggest, trace=0)
fwd.prediction <- predict(fit.fwd, newx = x_test)
summary(fit.fwd)
```

## 6-5 Stepwise Regression
The number of independent variable : 35 -> 26
```{r}
min.model <- lm(position_numeric ~ 1,data=data_train)
full.model <- lm(position_numeric~.,data=data_train)
fit.stepwise <- step(min.model, scope = list(upper=full.model), direction = "both", data=data_train, trace=0)
stepwise.prediction <- predict(fit.stepwise, newx = x_test)
summary(fit.stepwise)
```

## 6-6 Polynomial Regression (degree=2)
```{r}
x <- data_train[,-1]
fit.poly <- lm(position_numeric ~ poly(x[,1], degree=2, raw=TRUE)
                               + poly(x[,2], degree=2, raw=TRUE)
                               + poly(x[,3], degree=2, raw=TRUE)
                               + poly(x[,4], degree=2, raw=TRUE)
                               + poly(x[,5], degree=2, raw=TRUE)
                               + poly(x[,6], degree=2, raw=TRUE)
                               + poly(x[,7], degree=2, raw=TRUE)
                               + poly(x[,8], degree=2, raw=TRUE)
                               + poly(x[,9], degree=2, raw=TRUE)
                               + poly(x[,10], degree=2, raw=TRUE)
                               + poly(x[,11], degree=2, raw=TRUE)
                               + poly(x[,12], degree=2, raw=TRUE)
                               + poly(x[,13], degree=2, raw=TRUE)
                               + poly(x[,14], degree=2, raw=TRUE)
                               + poly(x[,15], degree=2, raw=TRUE)
                               + poly(x[,16], degree=2, raw=TRUE)
                               + poly(x[,17], degree=2, raw=TRUE)
                               + poly(x[,18], degree=2, raw=TRUE)
                               + poly(x[,19], degree=2, raw=TRUE)
                               + poly(x[,20], degree=2, raw=TRUE)
                               + poly(x[,21], degree=2, raw=TRUE)
                               + poly(x[,22], degree=2, raw=TRUE)
                               + poly(x[,23], degree=2, raw=TRUE)
                               + poly(x[,24], degree=2, raw=TRUE)
                               + poly(x[,25], degree=2, raw=TRUE)
                               + poly(x[,26], degree=2, raw=TRUE)
                               + poly(x[,27], degree=2, raw=TRUE)
                               + poly(x[,28], degree=2, raw=TRUE)
                               + poly(x[,29], degree=2, raw=TRUE)
                               + poly(x[,30], degree=2, raw=TRUE)
                               + poly(x[,31], degree=2, raw=TRUE)
                               + poly(x[,32], degree=2, raw=TRUE)
                               + poly(x[,33], degree=2, raw=TRUE)
                               + poly(x[,34], degree=2, raw=TRUE)
                               + poly(x[,35], degree=2, raw=TRUE), data=data_train)
poly.prediction <- predict(fit.poly, newx = x_test)
summary(fit.poly)
```

## 6-7 Polynomial Regression (degree=3)
```{r}
x <- data_train[,-1]
fit.poly2 <- lm(position_numeric ~ poly(x[,1], degree=3, raw=TRUE)
                               + poly(x[,2], degree=3, raw=TRUE)
                               + poly(x[,3], degree=3, raw=TRUE)
                               + poly(x[,4], degree=3, raw=TRUE)
                               + poly(x[,5], degree=3, raw=TRUE)
                               + poly(x[,6], degree=3, raw=TRUE)
                               + poly(x[,7], degree=3, raw=TRUE)
                               + poly(x[,8], degree=3, raw=TRUE)
                               + poly(x[,9], degree=3, raw=TRUE)
                               + poly(x[,10], degree=3, raw=TRUE)
                               + poly(x[,11], degree=3, raw=TRUE)
                               + poly(x[,12], degree=3, raw=TRUE)
                               + poly(x[,13], degree=3, raw=TRUE)
                               + poly(x[,14], degree=3, raw=TRUE)
                               + poly(x[,15], degree=3, raw=TRUE)
                               + poly(x[,16], degree=3, raw=TRUE)
                               + poly(x[,17], degree=3, raw=TRUE)
                               + poly(x[,18], degree=3, raw=TRUE)
                               + poly(x[,19], degree=3, raw=TRUE)
                               + poly(x[,20], degree=3, raw=TRUE)
                               + poly(x[,21], degree=3, raw=TRUE)
                               + poly(x[,22], degree=3, raw=TRUE)
                               + poly(x[,23], degree=3, raw=TRUE)
                               + poly(x[,24], degree=3, raw=TRUE)
                               + poly(x[,25], degree=3, raw=TRUE)
                               + poly(x[,26], degree=3, raw=TRUE)
                               + poly(x[,27], degree=3, raw=TRUE)
                               + poly(x[,28], degree=3, raw=TRUE)
                               + poly(x[,29], degree=3, raw=TRUE)
                               + poly(x[,30], degree=3, raw=TRUE)
                               + poly(x[,31], degree=3, raw=TRUE)
                               + poly(x[,32], degree=3, raw=TRUE)
                               + poly(x[,33], degree=3, raw=TRUE)
                               + poly(x[,34], degree=3, raw=TRUE)
                               + poly(x[,35], degree=3, raw=TRUE), data=data_train)
poly2.prediction <- predict(fit.poly2, newx = x_test)
summary(fit.poly2)
```

## 6-8 Polynomial Regression (degree=8)
```{r}
x <- data_train[,-1]
fit.poly3 <- lm(position_numeric ~ poly(x[,1], degree=8, raw=TRUE)
                               + poly(x[,2], degree=8, raw=TRUE)
                               + poly(x[,3], degree=8, raw=TRUE)
                               + poly(x[,4], degree=8, raw=TRUE)
                               + poly(x[,5], degree=8, raw=TRUE)
                               + poly(x[,6], degree=8, raw=TRUE)
                               + poly(x[,7], degree=8, raw=TRUE)
                               + poly(x[,8], degree=8, raw=TRUE)
                               + poly(x[,9], degree=8, raw=TRUE)
                               + poly(x[,10], degree=8, raw=TRUE)
                               + poly(x[,11], degree=8, raw=TRUE)
                               + poly(x[,12], degree=8, raw=TRUE)
                               + poly(x[,13], degree=8, raw=TRUE)
                               + poly(x[,14], degree=8, raw=TRUE)
                               + poly(x[,15], degree=8, raw=TRUE)
                               + poly(x[,16], degree=8, raw=TRUE)
                               + poly(x[,17], degree=8, raw=TRUE)
                               + poly(x[,18], degree=8, raw=TRUE)
                               + poly(x[,19], degree=8, raw=TRUE)
                               + poly(x[,20], degree=8, raw=TRUE)
                               + poly(x[,21], degree=8, raw=TRUE)
                               + poly(x[,22], degree=8, raw=TRUE)
                               + poly(x[,23], degree=8, raw=TRUE)
                               + poly(x[,24], degree=8, raw=TRUE)
                               + poly(x[,25], degree=8, raw=TRUE)
                               + poly(x[,26], degree=8, raw=TRUE)
                               + poly(x[,27], degree=8, raw=TRUE)
                               + poly(x[,28], degree=8, raw=TRUE)
                               + poly(x[,29], degree=8, raw=TRUE)
                               + poly(x[,30], degree=8, raw=TRUE)
                               + poly(x[,31], degree=8, raw=TRUE)
                               + poly(x[,32], degree=8, raw=TRUE)
                               + poly(x[,33], degree=8, raw=TRUE)
                               + poly(x[,34], degree=8, raw=TRUE)
                               + poly(x[,35], degree=8, raw=TRUE), data=data_train)
poly3.prediction <- predict(fit.poly3, newx = x_test)
summary(fit.poly3)
```

## 6-9 Lasso Regression
Implementation of the feature Selection via Lasso Regression
The number of independent variable : 35 -> 32
```{r}
library(glmnet)
set.seed(5555)
fit.lasso <- glmnet(x1, y1, alpha=1, family="gaussian")
fit.lasso.cv <- cv.glmnet(x1, y1, alpha=1, nfolds=10, type.measure="mse", family="gaussian")
lasso.coef = predict(fit.lasso, type = "coefficients", s=fit.lasso$lambda.min)
lasso.coef.cv = predict(fit.lasso.cv, type = "coefficients", s=fit.lasso.cv$lambda.min) # coefficients
lasso.prediction = predict(fit.lasso, s=fit.lasso$lambda.min, newx = x2) 
lasso.prediction.cv = predict(fit.lasso.cv, s=fit.lasso.cv$lambda.min, newx = x2) # coefficients
print(lasso.coef.cv)
 
# summary(fit.lasso)
summary(fit.lasso.cv)
```


## 6-10 Ridge Regression 
The number of independent variable : 35 -> 35
```{r}
# Ridge regression
set.seed(6666)
fit.ridge <- glmnet(x1, y1, alpha=0, family="gaussian")
fit.ridge.cv <- cv.glmnet(x1, y1, alpha=0, nfolds=10, type.measure="mse", family="gaussian")
ridge.coef = predict(fit.ridge, type = "coefficients", s=fit.ridge$lambda.min)
ridge.coef.cv = predict(fit.ridge.cv, type = "coefficients", s=fit.ridge.cv$lambda.min) # coefficients
ridge.prediction = predict(fit.ridge, s=fit.ridge$lambda.min, newx = x2)
ridge.prediction.cv = predict(fit.ridge.cv, s=fit.ridge.cv$lambda.min, newx = x2) # coefficients
print(ridge.coef.cv)
 
# summary(fit.ridge)
summary(fit.ridge.cv)
```




## 6-11 ElasticNet Regression 
The number of independent variable : 35 -> 28
```{r}
# ELASTIC NET WITH 0 < ALPHA < 1
set.seed(8888)
a3 <- seq(0.02, 0.98, 0.02)
search <- foreach(i = a3, .combine = rbind) %dopar% {
  cv <- cv.glmnet(x1, y1, alpha = i, nfold = 10, type.measure = "mse", family="gaussian")
  data.frame(cvm = cv$cvm[cv$lambda == cv$lambda.min], lambda.1se = cv$lambda.1se, alpha = i)
}
cv3 <- search[search$cvm == min(search$cvm), ]
fit.elnet <- glmnet(x1, y1, alpha = cv3$alpha, family="gaussian")
fit.elnet.cv <- cv.glmnet(x1, y1, alpha = cv3$alpha, nfolds=10, type.measure="mse", family="gaussian")
elnet.coef = predict(fit.elnet, type = "coefficients", s=fit.elnet$lambda.min)
elnet.coef.cv = predict(fit.elnet.cv, type = "coefficients", s=fit.elnet.cv$lambda.min) # coefficients
elnet.prediction = predict(fit.elnet, s=fit.elnet$lambda.min, newx = x2)
elnet.prediction.cv = predict(fit.elnet.cv, s=fit.elnet.cv$lambda.min, newx = x2) # coefficients
# coef(fit.elnet)
coef(fit.elnet.cv)
 
# summary(fit.elnet)
summary(fit.elnet.cv)
```


## 6-12 Model Performance Metrics for all models.

R-squared (R2), which is the proportion of variation in the outcome that is explained by the predictor variables. 
In multiple regression models, R2 corresponds to the squared correlation between the observed outcome values and the predicted values by the model. The Higher the R-squared, the better the model.

Root Mean Squared Error (RMSE), which measures the average error performed by the model in predicting the outcome for an observation. Mathematically, the RMSE is the square root of the mean squared error (MSE), which is the average squared difference between the observed actual outome values and the values predicted by the model. So, MSE = mean((observeds - predicteds)^2) and RMSE = sqrt(MSE). The lower the RMSE, the better the model.

Mean Absolute Error (MAE), like the RMSE, the MAE measures the prediction error. Mathematically, it is the average absolute difference between observed and predicted outcomes, MAE = mean(abs(observeds - predicteds)). MAE is less sensitive to outliers compared to RMSE.

```{r}
# MSE = mean((observeds - predicteds)^2)
# RMSE = sqrt(MSE)
# MAE = mean(abs(observeds - predicteds))
performance <- data.frame(
        MODEL_NAME = c("Linear", "Backward", "Forward", "Stepwise", "Polynomial(degree=2)"
                       , "Polynomial(degree=3)", "Polynomial(degree=8)", "Lasso", "Lidge", "ElasticNet")
        ,  
          
        RMSE = c(sqrt(mean((y_test - lm.prediction)^2)),
            sqrt(mean((y_test - bwd.prediction)^2)),
            sqrt(mean((y_test - fwd.prediction)^2)),
            sqrt(mean((y_test - stepwise.prediction)^2)),
            sqrt(mean((y_test - poly.prediction)^2)),
            sqrt(mean((y_test - poly2.prediction)^2)),
            sqrt(mean((y_test - poly3.prediction)^2)),
            sqrt(mean((y_test - lasso.prediction)^2)),
            sqrt(mean((y_test - ridge.prediction)^2)),
            sqrt(mean((y_test - elnet.prediction)^2)))
        ,
            
        MAE = c(mean(abs(y_test - lm.prediction)),
            mean(abs(y_test - bwd.prediction)),
            mean(abs(y_test - fwd.prediction)),
            mean(abs(y_test - stepwise.prediction)),
            mean(abs(y_test - poly.prediction)),
            mean(abs(y_test - poly2.prediction)),
            mean(abs(y_test - poly3.prediction)),
            mean(abs(y_test - lasso.prediction)),
            mean(abs(y_test - ridge.prediction)),
            mean(abs(y_test - elnet.prediction)))
        )
print(performance)
```
The result is that the Lasso Recession model has the smallest value in RMSE and MAE.



## 6-13 RMSE on test set with 10-Folds Cross Validation of Lasso, Lidge, ElasticNet.
```{r}
# RMSE on test set
estimation <- data.frame(
  regression = c("Lasso", "Lidge", "ElasticNet"),
  RMSE = c(sqrt(mean((y2 - lasso.prediction.cv)^2)), sqrt(mean((y2 - ridge.prediction.cv)^2)), 
          sqrt(mean((y2 - elnet.prediction.cv)^2))),
  MAE = c(mean(abs(y2 - lasso.prediction.cv)), mean(abs(y2 - ridge.prediction.cv)), 
          mean(abs(y2 - elnet.prediction.cv)))
  ) 
estimation
```
The result is that the ElasticNet Recession model has the smallest value in RMSE.



## 6-14 visualization of LASSO, Ridge, Elastic Net
Plot solution path and cross-validated MSE as function of λ.
```{r}
# Plot solution paths:
par(mfrow=c(3,2))
# For plotting options, type '?plot.glmnet' in R console
plot(fit.lasso, xvar="lambda", main="LASSO")
plot(fit.lasso.cv, main="LASSO")
plot(fit.ridge, xvar="lambda", main="Ridge")
plot(fit.ridge.cv, main="Ridge")
plot(fit.elnet, xvar="lambda", main="Elastic Net")
plot(fit.elnet.cv, main="Elastic Net")
```

## 6-15 Visualization of Lasso and ElasticNet Regresstion Model
```{r}
par(mfrow=c(1,2))
# Lasso predicted and observed
plot(y2, lasso.prediction.cv, ylim=c(min(lasso.prediction.cv), max(lasso.prediction.cv)), xlim=c(min(y2), max(y2)),main="Test Dataset", xlab="observed", ylab="lasso Predicted")
abline(0, 1, col="red")
# ElasticNet predicted and observed
plot(y2, elnet.prediction.cv, ylim=c(min(elnet.prediction.cv), max(elnet.prediction.cv)), xlim=c(min(y2), max(y2)),main="Test Dataset", xlab="observed", ylab="ElasticNet Predicted")
abline(0, 1, col="red")
```



## 6-16 Apply my soccer score with all regression models.
recommend my soccer position with all regression models.
```{r}
# prepare my dataset for recommendation of my soocer position. 
# create dataset
me_test <- x_test
# input my soccer score
me_test$height[1] <- 173
me_test$weight[1] <- 171
me_test$preferred_foot[1] <- 2
me_test$crossing[1] <- 80
me_test$finishing[1] <- 85
me_test$heading_accuracy[1] <- 60
me_test$short_passing[1] <- 90
me_test$volleys[1] <- 75
me_test$dribbling[1] <- 95
me_test$curve[1] <- 85
me_test$free_kick_accuracy[1] <- 80
me_test$long_passing[1] <- 90
me_test$ball_control[1] <- 95
me_test$acceleration[1] <- 50
me_test$sprint_speed[1] <- 30
me_test$agility[1] <- 70
me_test$reactions[1] <- 80
me_test$balance[1] <- 75
me_test$shot_power[1] <- 85
me_test$jumping[1] <- 40
me_test$stamina[1] <- 40
me_test$strength[1] <- 75
me_test$long_shots[1] <- 85
me_test$aggression[1] <- 90
me_test$interceptions[1] <- 65
me_test$positioning[1] <- 80
me_test$vision[1] <- 95
me_test$marking[1] <- 60
me_test$standing_tackle[1] <- 30
me_test$sliding_tackle[1] <- 25
me_test$gk_diving[1] <- 7
me_test$gk_handling[1] <- 7
me_test$gk_kicking[1] <- 12
me_test$gk_positioning[1] <- 7
me_test$gk_reflexes[1] <- 7
# convert dataframe to matrix.
me_test_matrix <- as.matrix(me_test)
# recommendation of my soccer position with all regresstion models.
set.seed(99999)
lm.prediction.me <- predict(fit.lm, newx = me_test_matrix)
bwd.prediction.me <- predict(fit.bwd, newx = me_test_matrix)
fwd.prediction.me <- predict(fit.fwd, newx = me_test_matrix)
stepwise.prediction.me <- predict(fit.stepwise, newx = me_test_matrix)
poly.prediction.me <- predict(fit.poly, newx = me_test_matrix)
poly2.prediction.me <- predict(fit.poly2, newx = me_test_matrix)
poly3.prediction.me <- predict(fit.poly3, newx = me_test_matrix)
lasso.prediction.me <- predict(fit.lasso, s=fit.lasso$lambda.min, newx = me_test_matrix) 
lasso.prediction.cv.me <- predict(fit.lasso.cv, s=fit.lasso.cv$lambda.min, newx = me_test_matrix)
ridge.prediction.me <- predict(fit.ridge, s=fit.ridge$lambda.min, newx = me_test_matrix)
ridge.prediction.cv.me <- predict(fit.ridge.cv, s=fit.ridge.cv$lambda.min, newx = me_test_matrix)
elnet.prediction.me <- predict(fit.elnet, s=fit.elnet$lambda.min, newx = me_test_matrix)
elnet.prediction.cv.me <- predict(fit.elnet.cv, s=fit.elnet.cv$lambda.min, newx = me_test_matrix)
# result of recommendation my soccer position with all regresstion models.
recom_myposition <- data.frame(
        Linear <- lm.prediction.me[1], 
        Backward <- bwd.prediction.me[1],
        Forward <- fwd.prediction.me[1], 
        Stepwise <- stepwise.prediction.me[1],
        Polynomial1 <- poly.prediction.me[1],
        Polynomial2 <- poly2.prediction.me[1],
        Polynomial3 <- poly3.prediction.me[1], 
        Lasso <- poly3.prediction.me[1], 
        Lasso.cv <- lasso.prediction.me[1],
        Lasso  <- lasso.prediction.cv.me[1], 
        Lidge  <- ridge.prediction.me[1], 
        Lidge.cv  <- ridge.prediction.cv.me[1], 
        ElasticNet  <- elnet.prediction.me[1], 
        ElasticNet.cv <- elnet.prediction.cv.me[1])
# create dataframe of result.
recom_myposition <- data.frame(
          c("Linear", "Backward", "Forward", "Stepwise", "Polynomial1",
            "Polynomial2", "Polynomial3", "Lasso", "Lasso.cv", "Lidge", "Lidge.cv",
            "ElasticNet", "ElasticNet.cv"),
  
           c(lm.prediction.me[1], 
            bwd.prediction.me[1],
            fwd.prediction.me[1], 
            stepwise.prediction.me[1],
            poly.prediction.me[1],
            poly2.prediction.me[1],
            poly3.prediction.me[1], 
            lasso.prediction.me[1],
            lasso.prediction.cv.me[1], 
            ridge.prediction.me[1], 
            ridge.prediction.cv.me[1], 
            elnet.prediction.me[1], 
            elnet.prediction.cv.me[1]))
colnames(recom_myposition) <- c("Regression", "My position recommend")
recom_myposition
 
```
I am satisfied with the results of Lasso and Lidge and ElasticNet regression models.
Among them, I was most satisfied with the result of using ElasticNet.cv model.
Because it is my preferred soccer position.
So I will choose a ElasticNet regression model using 10-Folds cv.



# 7. Summary
First, we preprocessed the dataset with removed missing value, unnecessary variables, convering variable type.
And then, we checked linear model assumptions such as Normality, Independence, Heteroscedasticity, Multicolinearity.
The dataset does not meet the assumption of a linear model and it also has multicollinearity problems. 
In order to increase performance, we used some regresstion models such as Backward Regression, Forward Regression,
Stepwise Regression, Polynomial Regression, Lasso Regression, Ridge Regression, ElasticNet Regression.
And we evaluated all regression models with RMSE, MAE. 
The result is that the Lasso, ElasticNet Recession models has the smallest value in RMSE and MAE.
Finally, I input my soccer position information to all regression models for predicton of my position.
The results were satisfied only on some models.
Among them, I was most satisfied with the result of using ElasticNet.cv model.
Because it matches my preferred soccer position,



# 8. Conclusion
In order to provide soccer position recommend service to the clients,
We will choose a ElasticNet regression model with 10-Folds cv.

Because it has the best performance among regression models. 
And the elastic net model is that it enables effective regularization 
via the ridge penalty with the feature selection characteristics of the lasso penalty. 
Effectively, elastic nets allow us to control multicollinearity concerns, 
perform regression when p>n, and reduce excessive noise in our data so that
we can isolate the most influential variables while balancing prediction accuracy.

However, elastic nets, and regularization models in general, still assume linear relationships 
between the features and the target variable. 
And although we can incorporate non-additive models with interactions, 
doing this when the number of features is large is extremely tedious and difficult. 
When non-linear relationships exist, its beneficial to start exploring non-linear regression approaches.



# 9. Suggestion for our Clients
Our clients are all people who love soccer. If you want to know your soccer position for your soccer match.
You can use our service that recommend soccer position if you input your information data to our service.
This service has machine learning technology using ElasticNet regression model. 
And also this service has an average error of 4.651255(MAE). 
Is there a service with better performance? Choose our sevice. You will not regret it.
